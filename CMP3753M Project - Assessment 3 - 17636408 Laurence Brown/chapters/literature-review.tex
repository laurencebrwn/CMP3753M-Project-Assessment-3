\section{Background}
As the COVID-19 pandemic has been prevalent for two years now, there has been a significant effort to compile medical data from across the globe to create large datasets for analysis and projects such as this one. The current COVID-19 datasets can be split into 4 main categories, the first being medical data and population sentiment as text, one such example of this is the “COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University” \citep{dong2020interactive}. This dataset logs the case numbers, deaths and recoveries by location to a daily granularity and can be used to predict the spread of the disease across the world. This data is not, however, useful for predicting whether someone has COVID-19, therefore, the next three data sets, that provided more relevant data, were considered for this project.

The first is acoustic data from recorded coughs and breathing of people with COVID. The COUGHVID crowdsourcing dataset “provides over 25,000 crowdsourced cough recordings representing a wide range of participant ages, genders, geographic locations, and COVID-19 statuses” \citep[pg. 1]{orlandic2021coughvid}. While the dataset is large and diverse, there are several reasons why it was not selected for use in evaluating models. Firstly, cough recordings can be hard to produce from a patient, for example if the patient is in a coma or on a ventilator. Secondly, research already conducted into using machine learning algorithms to diagnose COVID-19 from this dataset has a relatively low best accuracy of ~0.72 \citep{chang2021covnet}. 

The second of the three key datasets are those that catalog CT-Scan images from hospitals across the world, one notable dataset is COVIDx-CT, which contains over 100,000 CT slices across 1,489 patients \citep{gunraj2020covidnet}. While the leading model trained on the dataset, obtained accuracy results of 99.1\%, CT imaging technology was not selected for use in this evaluation. This was due to the size of the data set (three times the size of the finally selected one), the reduced number of patients (over ten times less than the selected data set) and due to the limited computing power available.

The final dataset category is X-Ray scans, with databases cataloging chest scans of patients both with and without COVID-19. The most popular being COVIDx-CXR, a large dataset of chest X-Ray images of over 16,000 patients \citep{wang2020covid}. The COVIDx-CXR dataset is the selected dataset for this study. It was chosen for the following reasons: It has been used successfully in numerous studies investigating machine learning algorithms at diagnosis of COVID-19; it has a large number of patients to train models on; and the size of the data set was likely to work within the resource constraints of the project (i.e. availability of computing power and time). The COVIDx-CXR dataset has two label sets, one for binary classification (normal/COVID-19 positive) and one for multilabel classification (normal/non-COVID-19 pneumonia/COVID-19 pneumonia). I will use the binary classification for model training and evaluation, specifically as it can be directly compared with sensitivity and specificity of LFT and PCR tests. Using binary classification also helps reduce computational load, to mitigate the risk set out during the proposal of the project. However, other models and studies that make use of COVID-19 X-Ray datasets report results using multilabel classification. This means that it might be required to train the best model found from my evaluation on the multilabel classification set to compare results with other papers.

\section{Related Literature}
One such paper that addresses the effectiveness of transfer learning models on classification of normal / non-COVID pneumonia / COVID pneumonia is “Comparing different deep learning architectures for classification of chest radiographs” \citep{bressem2020comparing}. The paper discusses the results of “15 different convolutional neural networks (CNN) of five different architectures (ResNet, DenseNet, VGG, SqueezeNet, Inception v4 and AlexNet)”, with its best performing networks achieving an AU-ROC score of 0.998. The study is also careful to state what datasets they are using, class counts and their training and testing methodology. However, the paper offers no insight as to what, if any, hyper parameters and model tuning was per-formed, and which of those parameters were used to generate the results. 

On the contrary, the paper “ COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images” offers an in-depth explanation to the model parameters used during the evaluation of several models when classifying COVID-19 cases \citep{zhang2020covid19xraynet}. The paper’s final models are high achieving with accuracies of 0.91 and above, however, when preparing the dataset for use, \cite{zhang2020covid19xraynet} states, “This study utilized two datasets for training”, one dataset for normal patients and patients with pneumonia, and another for those with COVID-19. However the first dataset only consists of X-Rays of people aged 1-5 years, the second had no denoted age, and mitigation strategies were not discussed on prevent models from distinguishing between young and old patients, compared to those with and without COVID-19, possibly invalidating findings.

The paper “Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans” \citep{roberts2021common}, sums up all of the above pitfalls, and more, found in 62 COVID-19 machine learning studies. This study will aim to take many of the pitfalls found by Roberts, et al. and, through a reasoned and justified approach to model evaluation, account for these errors to provide real-world validity in the found results. Key findings in \cite{roberts2021common} review in detail how the majority of studies fail to show how to reproduce exact results. This is an issue when it comes to implementing a solution with real world applicability, an example of this is the \cite{bressem2020comparing} study mentioned earlier. Another key finding is that studies fail to use machine learning best practices, very few studies detail how hyperparameter tuning was performed, if at all, and the \cite{bressem2020comparing} study fails to implement cross validation. Rather than eliminating dataset bias by taking a mean of results with different segments of the dataset used as validation, the study takes a crude mean of training the model on the same train, validation, test split to iron out minor fluctuations. \cite{roberts2021common} also finds that there is little external validation to justify applicability of methods showcased in papers. One method of doing this would be to compare model results to those of PCR and LFTs, so to justify the applicability of models in the real world, this study aims to do so. Finally, the majority of studies reviewed by \cite{roberts2021common} make use of transfer learning only, with no discussion on what the results might be if models where trained from scratch, exclusively on a COVID-19 dataset. This project aims to address this issue by comparing both transfer learning models and “train from scratch” models.
