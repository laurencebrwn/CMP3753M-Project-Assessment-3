This section aims to give a critical reflection of my own thoughts on the project. While many aspects of the project went well, some could have been improved if undertaken again. To aid in this analysis,segments of the Gibbs reflective cycle were used to help structure this section \citep{gibbs1988learning}.

\section{Positive Aspects of the Project}
Many aspects of this project were positive, the first of which is the development of the artefact itself. Throughout the duration of this project I built a solid framework in which to train and evaluate many models and variations quickly and efficiently. When beginning the project I dedicated a lot of time to learning the various libraries involved and methods that other researchers used to carry out such evaluations, this helped significantly in building this robust artefact. The artefact can be used in the future if necessary, for further evaluations in this space or to re-apply in other projects.

Another aspect of this project that went well was the model performance itself. My expectations going into this project were that it would be unachievable to build a model as successful as other published papers. The results of the best models in the study speak for themselves and with the excellent performance metrics of the final model, I feel very accomplished having even beaten other studies model performance. This is not to say that the model performances achieved in this study are unrivalled or ground breaking, but being able to judge my results on par with others results was a success in itself.

I was also able to gain a deep understanding of machine learning, deep learning and the toolsets required to enable these. Learning the workings of libraries like Keras and TensorFlow has served me well for not only this project, but will help me in the future by being able to apply this knowledge to the industry. It also enabled me to quickly recreate other researchers models from architecture diagrams, like \cite{fitriasari2021improvement} Xception-ResNet model. Using Google Cloud Platform as a development environment was a particularly beneficial part to this project, as it saved days worth of training time by enabling me to train models in parallel, by splitting execution tasks among many virtual servers.

Finally the analysis of the models went smoothly, by devising the entire project plan I would use beforehand within the methodology and applying this in my evaluation allowed me to quickly judge and compare models in this study and others critically. 

\section{Negative Aspects of the Project}
While a lot of the project went well there were some items that did not. Inconsistencies of metric recording is one example. While I had a methodology in place of which metrics I would use to evaluate the performance, there were some discrepancies on where that data came from. It is accepted that model performance can be taken from the mean results of cross validation on the validation dataset or the best performance on the test dataset from the folds used in cross validation. During the first round of model testing I had not known this, so the initial models were judged by their mean scores on each five folds performance on the test dataset. Following on from this I made sure to record all validation and test data for each fold individually. I used this for reference, but the mean test metrics were still used as the main metric. Other studies, for example \cite{bressem2020comparing}, also used a similar format of metric recording so I was still able to draw comparisons however. By using a uniform method to record every metric from each fold, this would not have been much of an issue.

Another portion of the project that did not go as intended was the model improvements. While I managed to learn about what modifications did not work well and what modifications reduced training time, I was not able to improve Xception's performance beyond the original modifications I had made. Perhaps it might have been better to evaluate all of the models in the initial round with no modifications whatsoever, except those required to fit the dataset image size and number of classes. Then I could have used my initial modification within the improvements round, and worked off of that instead. Another thing that might have helped resolve this was to explore more literature during the first phase of the project that focused on machine learning and deep learning network modification for performance. I could have then taken this onto the model improvements round and tested any theories I might have come up with from literature research.

The final major negative aspect is one that was beyond control, but identified within section \ref{risks}, was the need for significant computational performance for larger algorithms. While the models selected were efficient when discussed in literature, for example \cite{KerasApp92:online}, I did not take into account the time taken for hyper-parameter tuning and the size of the COVIDx-CXR dataset. While I performed some mitigation to reduce over-running on time during the project for training, it still took longer than I had hoped. I was able to use a reduced sample of COVIDx-CXR, as I reffered to as the "small dataset", this reduced training time during initial rounds. I was also able to use Google Cloud Platform to train models in parrallel, which again reduced time for training. The fact that each model had to train against a dataset five times for cross validation, multiplied by the number of hyper-parameter configurations for each network, it still took a significant amount of time. In the end I depleted the "free \$300 trial credits" within Google Cloud Platform right after initial training. This meant I had to rely on Google Colab for the remainder of the project, which had a limited run time of 12 hours per notebook. Fortunately the longest training time of the models (Xception - Original Improvements, on the large dataset) ran for just under this amount of time. In hindsight, I could have secured funding or planned computational requirements beforehand to reduce the chance of over-running on time for this section of the project.

\section{Improvements That Could Have Been Made}
There were several items in this project that had not been performed due to time constraints of the project, that if there had not been constraints I would have performed. The first of which is drawing further inspiration from other COVID-19 classification models. While I did do this during the end of the improvements phase, using \cite{fitriasari2021improvement} Xcpetion-ResNet model, I feel as though more improvements could have been made had I had the time to further research COVID-19 classification studies and recreate others models for myself. As well as aiding in improvements it would also allow me to evaluate performance of other studies models on the same dataset, adding an extra layer of fairness to the project.

Additionally, trialling use of other pre-training methods like the \cite{bressem2020comparing} use of the CheXpert dataset to pre-train models before training on a COVID-19 dataset. Doing this would likely have further improved all models results. I could have also compared the effect of using pre-training methods like these versus no pre-training more directly, instead of assigning a group of networks to make use of pre-training and a group to not.

I would have also liked to have broken down into the Xception model itself and altered the networks layers to trail performance improvements, instead of just appending various layers. This would have given me the opportunity to learn more about the inner workings of the top models and figure out why they were so successful. It would have also possibly yielded better performance improvements over just appending additional layers.

Finally, if I had more time I would have liked to explore the method devised by \cite{kamimura2019neural} to transform the networks tested in this project from multilayered neural networks to ones without any hidden layers. By doing so I would not only gain an understanding of the layout and inner workings of the model itself, I would have been able to interpret the relation between the models input and outputs. This would have essentially allowed me to diagnose how the model classified an image and which segments or features of an image it paid close attention to come to its outcome. By doing this I would be able to ensure the model was picking up on the areas within the X-Rays that medical experts use to diagnose COVID-19, rather than items in an X-Ray that might be typical to a patient with COVID-19 (respirator, etc.).