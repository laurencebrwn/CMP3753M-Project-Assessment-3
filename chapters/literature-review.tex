\section{Background}
As the COVID-19 pandemic has been prevalent for two years now, there has been a significant effort to compile medical data from across the globe to create large datasets for analysis and projects such as this one. The current COVID-19 datasets can be split into four main categories, the first being medical data and population sentiment as text, one such example of this is the “COVID-19 Data Repository by the Center for Systems Science and Engineering (CSSE) at Johns Hopkins University” \citep{dong2020interactive}. This dataset logs the case numbers, deaths and recoveries by location to a daily granularity and can be used to predict the spread of the disease across the world. This data is not, however, useful for predicting whether someone has COVID-19, therefore, the next three data sets, that provide more relevant data, were considered for this project.

The first is acoustic data from recorded coughs and breathing of people with COVID-19. The COUGHVID crowd-sourcing dataset “provides over 25,000 crowd-sourced cough recordings representing a wide range of participant ages, genders, geographic locations, and COVID-19 statuses” \citep[pg. 1]{orlandic2021coughvid}. While the dataset is large and diverse, there are several reasons why it was not selected for use in evaluating models. Firstly, cough recordings can be hard to produce from a patient, for example if the patient is in a coma or on a ventilator. Secondly, research already conducted into using machine learning algorithms to diagnose COVID-19 from this dataset has a relatively low best accuracy of ~0.72 \citep{chang2021covnet}. 

The second of the three key datasets are those that catalogue CT-Scan images from hospitals across the world, one notable dataset is COVIDx-CT, which contains over 100,000 CT slices across 1,489 patients \citep{gunraj2020covidnet}. While the leading model trained on the dataset obtained accuracy results of 99.1\%, CT imaging technology was not selected for use in this evaluation. This was due to the size of the data set (three times the size of the finally selected one), the reduced number of patients (over ten times less than the selected data set) and due to the limited computing power available.

The final dataset category is X-Ray scans, with databases cataloguing chest scans of patients both with and without COVID-19. The most popular being COVIDx-CXR, a large dataset of chest X-Ray images of over 16,000 patients \citep{wang2020covid}. The COVIDx-CXR dataset is the selected dataset for this study. It was chosen for the following reasons: it has been used successfully in numerous studies investigating machine learning algorithms at diagnosis of COVID-19; it has a large number of patients to train models on; and the size of the data set was likely to work within the resource constraints of the project (i.e. availability of computing power and time). The COVIDx-CXR dataset has two label sets, one for binary classification (normal/COVID-19 positive) and one for multi-label classification (normal/non-COVID-19 pneumonia/COVID-19 pneumonia). This project will use the binary classification for model training and evaluation, specifically as it can be directly compared with sensitivity and specificity of LFT and PCR tests. Using binary classification also helps reduce computational load, to mitigate the risk set out in the methodology of this project (section \ref{risks}). However, other models and studies that make use of COVID-19 X-Ray datasets report results using multi-label classification. This means this project will be required to train the best model found from the evaluation on the multi-label classification set to compare results with other papers.

\section{Related Literature}
One paper that addresses the effectiveness of transfer learning models on classification of normal / non-COVID-19 pneumonia / COVID-19 pneumonia is “Comparing different deep learning architectures for classification of chest radiographs” \citep{bressem2020comparing}. This paper discusses the results of “15 different convolutional neural networks (CNN) of six different architectures (ResNet, DenseNet, VGG, SqueezeNet, Inception v4 and AlexNet)”, with its best performing networks achieving an AU-ROC score of 0.998. The study is also careful to state what datasets they are using, class counts and their training and testing methodology. However, the paper offers no insight as to what, if any, hyper parameters and model tuning was performed, and which of those parameters were used to generate the results. 

On the contrary, the paper “COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images” offers an in-depth explanation to the model parameters used during the evaluation of several models when classifying COVID-19 cases \citep{zhang2020covid19xraynet}. The paper’s final models are high achieving with accuracies of 0.91 and above, however, when preparing the dataset for use, \cite{zhang2020covid19xraynet} states, “This study utilised two datasets for training”, one dataset for normal patients and patients with pneumonia, and another for those with COVID-19. However the first dataset only consists of X-Rays of people aged 1-5 years, the second had no denoted age, and mitigation strategies were not discussed on preventing models from distinguishing between young and old patients, compared to those with and without COVID-19, possibly invalidating findings.

The paper “Common pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans” \citep{roberts2021common}, sums up all of the above pitfalls, and more, found in 62 COVID-19 machine learning studies. This study will aim to take many of the pitfalls found by \cite{roberts2021common} and, through a reasoned and justified approach to model evaluation, account for these errors to provide real-world validity in the found results. Key findings in \cite{roberts2021common} review in detail how the majority of studies fail to show how to reproduce exact results. This is an issue when it comes to implementing a solution with real world applicability, an example of this is the \cite{bressem2020comparing} study mentioned earlier. Another key finding is that studies fail to use machine learning best practices, very few studies detail how hyper-parameter tuning was performed, if at all, and the \cite{bressem2020comparing} study fails to implement cross validation. Rather than eliminating dataset bias by taking a mean of results with different segments of the dataset used as validation, the study takes a crude mean of training the model on the same train, validation and test split to iron out minor fluctuations. \cite{roberts2021common} also finds little external validation to justify applicability of methods showcased in papers. One method of doing this would be to compare model results to those of PCR and LFTs, so to justify the applicability of models in the real world, which this study aims to do. Finally, the majority of studies reviewed by \cite{roberts2021common} make use of transfer learning only, with no discussion on what the results might be if models were trained from scratch, exclusively on a COVID-19 dataset. This project aims to address this issue by comparing both transfer learning models and “train from scratch” models.

After algorithm research, considering best performing algorithms from other studies mentioned in this literature review, and recommendations from the \cite{roberts2021common} study, the following algorithms were selected for evaluation in this study:
\begin{itemize}
    \item AlexNet \citep{krizhevsky2012imagenet} – A CNN winning the ImageNet Large Scale Visual Recognition Challenge in 2012 and has shown very high accuracy results when trained on COVID-19 datasets. This model will be trained without any pre-existing weights, representing one of three models not using transfer learning.
    
    \item DenseNet201 \citep{huang2017densely} – A CNN which propagates knowledge from all previous layers in the CNN to each layer. This means errors can be shared with earlier layers directly and has a smaller size than other networks because of its connectivity. This model will be trained with pre-existing weights from training on the ImageNet dataset \citep{deng2009imagenet}, representing one of three models making use of transfer learning.
    
    \item ResNet50V2 \citep{he2016identity} – A deep residual network that makes use of skip connections to forgo diminishing error gradients when multiplied by each layer. The model won the 2015 ImageNet Large Scale Visual Recognition Challenge and shows some of the best results, on par with DenseNet, in the \cite{bressem2020comparing}. study. This model will be trained with pre-existing weights from training on the ImageNet dataset, representing one of three models making use of transfer learning.
    
    \item SqueezeNet \citep{iandola2016squeezenet} – A CNN which comprises “Fire Modules” to squeeze inputs into a 1x1 filter which results in a smaller network, that can be run on devices with limited memory, while retaining similar performance to AlexNet. This model will be trained without any pre-existing weights, representing one of three models not using transfer learning.
    
    \item Xception \citep{chollet2017xception} – A CNN 71 layers deep uses a modified depth-wise separable convolution, found in the Inception CNN, that uses no intermediate activation layers. Xception beats ResNet when training on the ImageNet dataset and shows some of the best results in the \cite{bressem2020comparing} study. This model will be trained with pre-existing weights from training on the ImageNet dataset, representing one of three models making use of transfer learning.
    
    \item ConvNet – A basic CNN created for this project using Keras, consisting of three convolutional layers. This model will be trained without any pre-existing weights, representing one of three models not using transfer learning.
\end{itemize}